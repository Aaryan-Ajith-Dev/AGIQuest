{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveHiringAGIEnv(gym.Env):\n",
    "    \"\"\"Competitive AGI Race Environment with Two Parties, Team Hiring, and Collaboration\"\"\"\n",
    "\n",
    "    def __init__(self, team_size=5, s0=4.0, alpha=1.5):\n",
    "        super(CompetitiveHiringAGIEnv, self).__init__()\n",
    "\n",
    "        # Team management parameters\n",
    "        self.team_size = team_size\n",
    "        self.max_skill = team_size\n",
    "        self.s0 = s0  # AGI threshold skill\n",
    "        self.alpha = alpha  # Sharpness of AGI probability curve\n",
    "        \n",
    "        # Defining collaboration modes\n",
    "        self.COLLAB_MODES = {\n",
    "            'INDEPENDENT': 0,   # Both parties working independently\n",
    "            'COLLABORATIVE': 1, # Both parties actively collaborating\n",
    "            'POST_COLLAB': 2,   # Both previously collaborated but now independent\n",
    "        }\n",
    "\n",
    "        # Defining party statuses\n",
    "        self.PARTY_STATUS = {\n",
    "            'EXPLORING': 0,     # Actively exploring\n",
    "            'RETREATED': 1,     # Retreated\n",
    "            'FOUND_AGI': 2,     # Found AGI\n",
    "        }\n",
    "\n",
    "        # Collaboration mode (applies to both parties)\n",
    "        self.collab_mode = self.COLLAB_MODES['INDEPENDENT']\n",
    "\n",
    "        # Individual party statuses\n",
    "        self.party_1_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_2_status = self.PARTY_STATUS['EXPLORING']\n",
    "\n",
    "        # Track which party has broken collaboration\n",
    "        self.party_1_broke_collab = False\n",
    "        self.party_2_broke_collab = False\n",
    "\n",
    "        # Track resources and teams\n",
    "        self.party_1_resources = 100\n",
    "        self.party_2_resources = 100\n",
    "        \n",
    "        # Initialize team members for each party\n",
    "        self.party_1_team = []\n",
    "        self.party_2_team = []\n",
    "        self.party_1_candidate = None\n",
    "        self.party_2_candidate = None\n",
    "        \n",
    "        # Team statistics\n",
    "        self.party_1_skill = 0\n",
    "        self.party_2_skill = 0\n",
    "        self.party_1_salary = 0\n",
    "        self.party_2_salary = 0\n",
    "\n",
    "        # Actions for each agent:\n",
    "        # 0 = Retreat (stop exploring)\n",
    "        # 1 = Explore independently\n",
    "        # 2 = Initiate/continue collaboration\n",
    "        # 3 = Break collaboration\n",
    "        # 4+ = Hire candidate (fire team member at index action-4)\n",
    "        self.action_space = spaces.Tuple((\n",
    "            spaces.Discrete(4 + team_size),  # Party 1 actions\n",
    "            spaces.Discrete(4 + team_size)   # Party 2 actions\n",
    "        ))\n",
    "\n",
    "        # The observation space includes:\n",
    "        # - Collaboration mode\n",
    "        # - Party statuses\n",
    "        # - Party resources\n",
    "        # - Team skills and salaries\n",
    "        # - Candidates\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"collab_mode\": spaces.Discrete(3),\n",
    "            \"party_1_status\": spaces.Discrete(3),\n",
    "            \"party_2_status\": spaces.Discrete(3),\n",
    "            \"party_1_resources\": spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            \"party_2_resources\": spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),\n",
    "            \"party_1_team_skills\": spaces.Box(low=0.0, high=1.0, shape=(team_size,), dtype=np.float32),\n",
    "            \"party_1_team_salaries\": spaces.Box(low=0.0, high=1.0, shape=(team_size,), dtype=np.float32),\n",
    "            \"party_2_team_skills\": spaces.Box(low=0.0, high=1.0, shape=(team_size,), dtype=np.float32),\n",
    "            \"party_2_team_salaries\": spaces.Box(low=0.0, high=1.0, shape=(team_size,), dtype=np.float32),\n",
    "            \"party_1_candidate\": spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32),\n",
    "            \"party_2_candidate\": spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        # Probabilities and rewards\n",
    "        self.independent_agi_prob_factor = 0.01  # Base prob, will be multiplied by team skill\n",
    "        self.collaborative_agi_prob_factor = 0.03\n",
    "        self.post_collab_agi_prob_factor = 0.015\n",
    "\n",
    "        # Resource dynamics\n",
    "        self.explore_independent_cost = 1\n",
    "        self.explore_collaborative_cost = 3\n",
    "        self.retreat_reward = 10\n",
    "        self.collaboration_initiation_cost = 5\n",
    "        self.agi_reward = 100\n",
    "        self.salary_cost_factor = 0.1  # Salary cost per step\n",
    "\n",
    "        # Competition penalty: if the opponent finds AGI first\n",
    "        self.competition_penalty = -50\n",
    "\n",
    "        # Max environment steps\n",
    "        self.max_steps = 1000\n",
    "        self.current_step = 0\n",
    "\n",
    "        ##########\n",
    "\n",
    "        # Defining the transition probabilities and rewards for each party\n",
    "        # Structure: {(party_status, collab_mode): {action: [{next_status, prob, reward, effects}, ...]}}\n",
    "        self.party_1_transitions = {\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['INDEPENDENT']): {\n",
    "                1: [  # Explore independently\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(1), \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_2_reward\", self.competition_penalty, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(1), \n",
    "                        \"reward\": -self.party_1_salary * self.salary_cost_factor, \"resource_change\": -self.explore_independent_cost, \"effects\": []}\n",
    "                ],\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Try to initiate collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                        \"reward\": -self.collaboration_initiation_cost, \"resource_change\": -self.collaboration_initiation_cost, \"effects\": []}\n",
    "                ],\n",
    "                # Hiring actions will be handled separately\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['COLLABORATIVE']): {\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Explore collaboratively\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(1), \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_collaborative_cost,\n",
    "                        \"effects\": [(\"party_2_status\", self.PARTY_STATUS['FOUND_AGI'], \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING']),\n",
    "                                (\"party_2_reward\", self.agi_reward, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(1), \n",
    "                        \"reward\": -(self.party_1_salary + self.party_2_salary) * self.salary_cost_factor , \"resource_change\": -self.explore_collaborative_cost, \"effects\": []}\n",
    "                ],\n",
    "                3: [  # Break collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                        \"reward\": 0, \"resource_change\": 0, \"effects\": [(\"party_1_broke_collab\", True, lambda: True)]}\n",
    "                ]\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['POST_COLLAB']): {\n",
    "                1: [  # Explore after collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(1), \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_2_reward\", self.competition_penalty, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(1), \n",
    "                        \"reward\": -(self.party_1_salary * self.salary_cost_factor), \"resource_change\": -self.explore_independent_cost, \"effects\": []}\n",
    "                ],\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Create similar transitions for party 2\n",
    "        # Create similar transitions for party 2 (properly swapping party references)\n",
    "        self.party_2_transitions = {\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['INDEPENDENT']): {\n",
    "                1: [  # Explore independently\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_1_reward\", self.competition_penalty,  # Changed party_2→party_1\n",
    "                                    lambda: self.party_1_status == self.PARTY_STATUS['EXPLORING'])]},  # Changed party_2→party_1\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": -self.party_2_salary * self.salary_cost_factor, \"resource_change\": -self.explore_independent_cost, \"effects\": []}\n",
    "                ],\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0,\n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Try to initiate collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0,\n",
    "                        \"reward\": -(self.collaboration_initiation_cost), \"resource_change\": -self.collaboration_initiation_cost, \"effects\": []}\n",
    "                ],\n",
    "                # Hiring actions will be handled separately\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['COLLABORATIVE']): {\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0,\n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Explore collaboratively\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_collaborative_cost,\n",
    "                        \"effects\": [(\"party_1_status\", self.PARTY_STATUS['FOUND_AGI'],  # Changed party_2→party_1\n",
    "                                    lambda: self.party_1_status == self.PARTY_STATUS['EXPLORING']),  # Changed party_2→party_1\n",
    "                                (\"party_1_reward\", self.agi_reward,  # Changed party_2→party_1\n",
    "                                    lambda: self.party_1_status == self.PARTY_STATUS['EXPLORING'])]},  # Changed party_2→party_1\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": -(self.party_1_salary + self.party_2_salary) * self.salary_cost_factor, \"resource_change\": -self.explore_collaborative_cost, \"effects\": []}\n",
    "                ],\n",
    "                3: [  # Break collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0,\n",
    "                        \"reward\": 0, \"resource_change\": 0, \"effects\": [(\"party_2_broke_collab\", True, lambda: True)]}  # Changed 1->2\n",
    "                ]\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['POST_COLLAB']): {\n",
    "                1: [  # Explore after collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": lambda: self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_1_reward\", self.competition_penalty,  # Changed party_2→party_1\n",
    "                                    lambda: self.party_1_status == self.PARTY_STATUS['EXPLORING'])]},  # Changed party_2→party_1\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": lambda: 1 - self._get_agi_probability(2),  # Changed 1→2\n",
    "                        \"reward\": -self.party_2_salary * self.salary_cost_factor, \"resource_change\": -self.explore_independent_cost, \"effects\": []}\n",
    "                ],\n",
    "                0: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0,\n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        #######\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _generate_worker(self, team_skill=0, skill_bias=0.5, skill_uncertainty=0.5, skill_mean=0.5, salary_noise=0.2):\n",
    "        \"\"\"Generate a worker with randomized skill and salary.\"\"\"\n",
    "        base_skill = np.clip(\n",
    "            np.random.normal(\n",
    "                loc=skill_mean + skill_bias * (team_skill / self.max_skill),\n",
    "                scale=skill_uncertainty\n",
    "            ), 0, 1\n",
    "        )\n",
    "\n",
    "        # Add noise to salary, making it only loosely correlated with skill\n",
    "        salary = np.clip(\n",
    "            base_skill + np.random.normal(loc=0.0, scale=salary_noise),\n",
    "            0.0,\n",
    "            1.0\n",
    "        )\n",
    "\n",
    "        return {\"skill\": base_skill, \"salary\": salary}\n",
    "\n",
    "    def _update_team_stats(self):\n",
    "        \"\"\"Update team statistics.\"\"\"\n",
    "        self.party_1_skill = sum(w['skill'] for w in self.party_1_team)\n",
    "        self.party_1_salary = sum(w['salary'] for w in self.party_1_team)\n",
    "        \n",
    "        self.party_2_skill = sum(w['skill'] for w in self.party_2_team)\n",
    "        self.party_2_salary = sum(w['salary'] for w in self.party_2_team)\n",
    "\n",
    "    # def _get_agi_probability(self, party_id):\n",
    "    #     \"\"\"Calculate probability of finding AGI based on team skill and collaboration mode.\"\"\"\n",
    "    #     team_skill = self.party_1_skill if party_id == 1 else self.party_2_skill\n",
    "    #     skill_factor = 1 / (1 + np.exp(-self.alpha * (team_skill - self.s0)))\n",
    "        \n",
    "    #     if self.collab_mode == self.COLLAB_MODES['INDEPENDENT']:\n",
    "    #         return skill_factor * self.independent_agi_prob_factor\n",
    "    #     elif self.collab_mode == self.COLLAB_MODES['COLLABORATIVE']:\n",
    "    #         return skill_factor * self.collaborative_agi_prob_factor\n",
    "    #     else:  # POST_COLLAB\n",
    "    #         return skill_factor * self.post_collab_agi_prob_factor\n",
    "    \n",
    "    def _get_agi_probability(self, party_id):\n",
    "        \"\"\"Calculate probability of finding AGI based on team skill and collaboration mode.\"\"\"\n",
    "        team_skill = self.party_1_skill if party_id == 1 else self.party_2_skill\n",
    "        skill_factor = 1 / (1 + np.exp(-self.alpha * (team_skill - self.s0)))\n",
    "        \n",
    "        if self.collab_mode == self.COLLAB_MODES['INDEPENDENT']:\n",
    "            return skill_factor * self.independent_agi_prob_factor\n",
    "        elif self.collab_mode == self.COLLAB_MODES['COLLABORATIVE']:\n",
    "            return skill_factor * self.collaborative_agi_prob_factor\n",
    "        else:  # POST_COLLAB\n",
    "            return skill_factor * self.post_collab_agi_prob_factor\n",
    "\n",
    "    def _process_collaboration_actions(self, action_1, action_2):\n",
    "        \"\"\"Process collaboration dynamics based on both parties' actions.\"\"\"\n",
    "        if self.collab_mode == self.COLLAB_MODES['INDEPENDENT']:\n",
    "            # Both parties agree to collaborate\n",
    "            if action_1 == 2 and action_2 == 2:\n",
    "                self.collab_mode = self.COLLAB_MODES['COLLABORATIVE']\n",
    "                self.party_1_resources -= self.collaboration_initiation_cost\n",
    "                self.party_2_resources -= self.collaboration_initiation_cost\n",
    "                return True\n",
    "                \n",
    "        elif self.collab_mode == self.COLLAB_MODES['COLLABORATIVE']:\n",
    "            # Breaking collaboration\n",
    "            if action_1 == 3 or action_2 == 3:\n",
    "                self.collab_mode = self.COLLAB_MODES['POST_COLLAB']\n",
    "                if action_1 == 3:\n",
    "                    self.party_1_broke_collab = True\n",
    "                if action_2 == 3:\n",
    "                    self.party_2_broke_collab = True\n",
    "                return True\n",
    "            \n",
    "            # Continuing collaboration\n",
    "            elif action_1 == 2 and action_2 == 2:\n",
    "                # Stay in collaboration mode\n",
    "                return True\n",
    "            # else:                                                                                 #COMMENTED THIS CODE BECAUSE OTHER ACTIONS SHOULDN'T BE ALLOWED IN THIS STAGE\n",
    "            #     # Implicit break by not continuing\n",
    "            #     self.collab_mode = self.COLLAB_MODES['POST_COLLAB']\n",
    "            #     return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "    def _decode_action(self, action):\n",
    "        \"\"\"Decode an action into explore/retreat/collaborate/hire operations.\"\"\"\n",
    "        if action < 4:\n",
    "            return {\"action_type\": action, \"fire_index\": None}\n",
    "        else:\n",
    "            return {\"action_type\": 4, \"fire_index\": action - 4}\n",
    "\n",
    "    def _process_party_actions(self, party_id, action_dict, reward):\n",
    "        \"\"\"Process a party's action and update state/reward using transition dictionaries.\"\"\"\n",
    "        status = self.party_1_status if party_id == 1 else self.party_2_status\n",
    "        \n",
    "        # Only process if party is still exploring\n",
    "        if status != self.PARTY_STATUS['EXPLORING']:\n",
    "            return reward\n",
    "        \n",
    "        action_type = action_dict[\"action_type\"]\n",
    "        \n",
    "        # Hiring action - handle separately since it's not in the transitions\n",
    "        if action_type == 4:\n",
    "            fire_index = action_dict[\"fire_index\"]\n",
    "            if party_id == 1 and 0 <= fire_index < self.team_size:\n",
    "                old_skill = self.party_1_team[fire_index][\"skill\"]\n",
    "                # Replace team member and update\n",
    "                self.party_1_team[fire_index] = self.party_1_candidate\n",
    "                new_skill = self.party_1_candidate[\"skill\"]\n",
    "                self.party_1_candidate = self._generate_worker(self.party_1_skill)\n",
    "                self._update_team_stats()\n",
    "                return reward + (new_skill - old_skill)\n",
    "                \n",
    "            elif party_id == 2 and 0 <= fire_index < self.team_size:\n",
    "                old_skill = self.party_2_team[fire_index][\"skill\"]\n",
    "                # Replace team member and update\n",
    "                self.party_2_team[fire_index] = self.party_2_candidate\n",
    "                new_skill = self.party_2_candidate[\"skill\"]\n",
    "                self.party_2_candidate = self._generate_worker(self.party_2_skill)\n",
    "                self._update_team_stats()\n",
    "                return reward + (new_skill - old_skill)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        # For exploration/collaboration/retreat, use transition dictionaries\n",
    "        transitions = None\n",
    "        if party_id == 1:\n",
    "            state_key = (self.party_1_status, self.collab_mode)\n",
    "            if state_key in self.party_1_transitions and action_type in self.party_1_transitions[state_key]:\n",
    "                transitions = self.party_1_transitions[state_key][action_type]\n",
    "        else:\n",
    "            state_key = (self.party_2_status, self.collab_mode)\n",
    "            if state_key in self.party_2_transitions and action_type in self.party_2_transitions[state_key]:\n",
    "                transitions = self.party_2_transitions[state_key][action_type]\n",
    "        \n",
    "        if transitions != None:\n",
    "            # Select transition based on probabilities\n",
    "            probs = [t[\"prob\"]() if callable(t[\"prob\"]) else t[\"prob\"] for t in transitions]        #MADE IT INTO A CALLABLE FUNCTION BECAUSE THERE IS LAMBDA FUNCTION THAT USES GET_AGI_PROBABILITY THIS TIME\n",
    "            transition_idx = np.random.choice(len(transitions), p=probs)\n",
    "            transition = transitions[transition_idx]\n",
    "            \n",
    "            # Apply transition\n",
    "            if party_id == 1:\n",
    "                self.party_1_status = transition[\"next_status\"]\n",
    "                self.party_1_resources += transition[\"resource_change\"]\n",
    "            else:\n",
    "                self.party_2_status = transition[\"next_status\"]\n",
    "                self.party_2_resources += transition[\"resource_change\"]\n",
    "            \n",
    "            reward += transition[\"reward\"]\n",
    "            \n",
    "            # Apply side effects\n",
    "            for effect in transition[\"effects\"]:\n",
    "                target, value, condition = effect\n",
    "                if condition():\n",
    "                    if target == \"party_1_reward\" and party_id == 2:\n",
    "                        reward += value\n",
    "                    elif target == \"party_2_reward\" and party_id == 1:\n",
    "                        # Apply to other party in the next step\n",
    "                        if self.party_2_status == self.PARTY_STATUS['EXPLORING']:\n",
    "                            self.party_2_resources += value\n",
    "                    elif target == \"party_1_status\" and party_id == 2:\n",
    "                        self.party_1_status = value\n",
    "                    elif target == \"party_2_status\" and party_id == 1:\n",
    "                        self.party_2_status = value\n",
    "                    elif target == \"party_1_broke_collab\":\n",
    "                        self.party_1_broke_collab = value\n",
    "                    elif target == \"party_2_broke_collab\":\n",
    "                        self.party_2_broke_collab = value\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Return the current observation (state).\"\"\"\n",
    "        return {\n",
    "            \"collab_mode\": self.collab_mode,\n",
    "            \"party_1_status\": self.party_1_status,\n",
    "            \"party_2_status\": self.party_2_status,\n",
    "            \"party_1_resources\": np.array([self.party_1_resources], dtype=np.float32),\n",
    "            \"party_2_resources\": np.array([self.party_2_resources], dtype=np.float32),\n",
    "            \"party_1_team_skills\": np.array([w['skill'] for w in self.party_1_team], dtype=np.float32),\n",
    "            \"party_1_team_salaries\": np.array([w['salary'] for w in self.party_1_team], dtype=np.float32),\n",
    "            \"party_2_team_skills\": np.array([w['skill'] for w in self.party_2_team], dtype=np.float32),\n",
    "            \"party_2_team_salaries\": np.array([w['salary'] for w in self.party_2_team], dtype=np.float32),\n",
    "            \"party_1_candidate\": np.array([self.party_1_candidate['skill'], self.party_1_candidate['salary']], dtype=np.float32),\n",
    "            \"party_2_candidate\": np.array([self.party_2_candidate['skill'], self.party_2_candidate['salary']], dtype=np.float32),\n",
    "        }\n",
    "\n",
    "    def _is_terminal(self):\n",
    "        \"\"\"Check if the episode has reached a terminal state.\"\"\"\n",
    "        return (self.party_1_status != self.PARTY_STATUS['EXPLORING'] or\n",
    "                self.party_2_status != self.PARTY_STATUS['EXPLORING'] or\n",
    "                self.current_step >= self.max_steps or\n",
    "                self.party_1_resources <= 0 or\n",
    "                self.party_2_resources <= 0)\n",
    "\n",
    "    def _get_terminal_info(self):\n",
    "        \"\"\"Gather information about how the episode ended.\"\"\"\n",
    "        info = {}\n",
    "\n",
    "        if self.party_1_resources <= 0:\n",
    "            info[\"bankrupt\"] = \"party_1\"\n",
    "        if self.party_2_resources <= 0:\n",
    "            info[\"bankrupt\"] = \"party_2\" if \"bankrupt\" not in info else \"both\"\n",
    "\n",
    "        if self.current_step >= self.max_steps:\n",
    "            info[\"timeout\"] = True\n",
    "\n",
    "        if self.party_1_status == self.PARTY_STATUS['FOUND_AGI'] and self.party_2_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"both\"\n",
    "        elif self.party_1_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"party_1\"\n",
    "        elif self.party_2_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"party_2\"\n",
    "\n",
    "        if \"winner\" not in info and \"bankrupt\" not in info and not info.get(\"timeout\", False):\n",
    "            # Someone retreated\n",
    "            if self.party_1_status == self.PARTY_STATUS['RETREATED'] and self.party_2_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"both_retreated\"] = True\n",
    "            elif self.party_1_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"retreated\"] = \"party_1\"\n",
    "            elif self.party_2_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"retreated\"] = \"party_2\"\n",
    "\n",
    "        # Add team statistics\n",
    "        info[\"party_1_skill\"] = self.party_1_skill\n",
    "        info[\"party_2_skill\"] = self.party_2_skill\n",
    "        info[\"party_1_salary\"] = self.party_1_salary\n",
    "        info[\"party_2_salary\"] = self.party_2_salary\n",
    "\n",
    "        return info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment with actions from both parties.\"\"\"\n",
    "        action_1, action_2 = action\n",
    "        \n",
    "        # Validate actions\n",
    "        assert 0 <= action_1 < 4 + self.team_size, f\"Invalid action for party 1: {action_1}\"\n",
    "        assert 0 <= action_2 < 4 + self.team_size, f\"Invalid action for party 2: {action_2}\"\n",
    "\n",
    "        # If already in terminal state, return without changes\n",
    "        if self._is_terminal():\n",
    "            return self._get_observation(), (0, 0), True, False, self._get_terminal_info()\n",
    "\n",
    "        # Process collaboration transitions first\n",
    "        self._process_collaboration_actions(action_1, action_2)\n",
    "\n",
    "        # Decode actions\n",
    "        action_dict_1 = self._decode_action(action_1)\n",
    "        action_dict_2 = self._decode_action(action_2)\n",
    "\n",
    "        #REMOVED initialised reward that were doing self.party_1_salary * self.salary_cost_factor as it is already handled in the transition matrix\n",
    "        reward_1 = 0\n",
    "        reward_2 = 0\n",
    "\n",
    "        # Process party actions and update rewards\n",
    "        reward_1 = self._process_party_actions(1, action_dict_1, reward_1)\n",
    "        reward_2 = self._process_party_actions(2, action_dict_2, reward_2)\n",
    "\n",
    "        # Apply competition penalty if one finds AGI and the other doesn't\n",
    "        if (self.party_1_status == self.PARTY_STATUS['FOUND_AGI'] and \n",
    "            self.party_2_status == self.PARTY_STATUS['EXPLORING']):\n",
    "            reward_2 += self.competition_penalty\n",
    "            \n",
    "        if (self.party_2_status == self.PARTY_STATUS['FOUND_AGI'] and \n",
    "            self.party_1_status == self.PARTY_STATUS['EXPLORING']):\n",
    "            reward_1 += self.competition_penalty\n",
    "\n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Determine if episode has ended\n",
    "        done = self._is_terminal()\n",
    "\n",
    "        # Gather additional info for terminal states\n",
    "        info = {}\n",
    "        if done:\n",
    "            info.update(self._get_terminal_info())\n",
    "\n",
    "        return self._get_observation(), (reward_1, reward_2), done, False, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.collab_mode = self.COLLAB_MODES['INDEPENDENT']\n",
    "        self.party_1_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_2_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_1_broke_collab = False\n",
    "        self.party_2_broke_collab = False\n",
    "        self.party_1_resources = 100\n",
    "        self.party_2_resources = 100\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Initialize teams\n",
    "        self.party_1_team = [self._generate_worker() for _ in range(self.team_size)]\n",
    "        self.party_2_team = [self._generate_worker() for _ in range(self.team_size)]\n",
    "        \n",
    "        # Generate initial candidates\n",
    "        self.party_1_candidate = self._generate_worker()\n",
    "        self.party_2_candidate = self._generate_worker()\n",
    "        \n",
    "        # Update team statistics\n",
    "        self._update_team_stats()\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Print the current state of the environment.\"\"\"\n",
    "        print(f\"Step: {self.current_step} | Collaboration Mode: {self.collab_mode}\")\n",
    "        print(f\"Party 1 Status: {self.party_1_status} | Resources: {self.party_1_resources:.2f}\")\n",
    "        print(f\"Party 2 Status: {self.party_2_status} | Resources: {self.party_2_resources:.2f}\")\n",
    "        print(f\"Party 1 Team Skill: {self.party_1_skill:.2f} | Salary: {self.party_1_salary:.2f}\")\n",
    "        print(f\"Party 2 Team Skill: {self.party_2_skill:.2f} | Salary: {self.party_2_salary:.2f}\")\n",
    "        print(\"Party 1 Team:\")\n",
    "        for i, worker in enumerate(self.party_1_team):\n",
    "            print(f\"  Worker {i}: Skill={worker['skill']:.2f}, Salary={worker['salary']:.2f}\")\n",
    "        print(\"Party 2 Team:\")\n",
    "        for i, worker in enumerate(self.party_2_team):\n",
    "            print(f\"  Worker {i}: Skill={worker['skill']:.2f}, Salary={worker['salary']:.2f}\")\n",
    "        print(f\"Party 1 Candidate: Skill={self.party_1_candidate['skill']:.2f}, Salary={self.party_1_candidate['salary']:.2f}\")\n",
    "        print(f\"Party 2 Candidate: Skill={self.party_2_candidate['skill']:.2f}, Salary={self.party_2_candidate['salary']:.2f}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiAgentDictObsPreprocessor() code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation preprocessor for the integrated environment\n",
    "class MultiAgentDictObsPreprocessor(nn.Module):\n",
    "    def __init__(self, team_size):\n",
    "        super().__init__()\n",
    "        self.team_size = team_size\n",
    "\n",
    "    def forward(self, obs_dict):\n",
    "        # Handle discrete observations with one-hot encoding\n",
    "        collab_mode = F.one_hot(torch.tensor(obs_dict[\"collab_mode\"], dtype=torch.long), num_classes=3).float()\n",
    "        party_1_status = F.one_hot(torch.tensor(obs_dict[\"party_1_status\"], dtype=torch.long), num_classes=3).float()\n",
    "        party_2_status = F.one_hot(torch.tensor(obs_dict[\"party_2_status\"], dtype=torch.long), num_classes=3).float()\n",
    "        \n",
    "        # Convert continuous observations to tensors\n",
    "        def ensure_2d(x, dtype=torch.float32):\n",
    "            x = torch.tensor(x, dtype=dtype)\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(0)  # convert [D] → [1, D]\n",
    "            return x\n",
    "\n",
    "        party_1_resources = ensure_2d(obs_dict[\"party_1_resources\"])\n",
    "        party_2_resources = ensure_2d(obs_dict[\"party_2_resources\"])\n",
    "        party_1_team_skills = ensure_2d(obs_dict[\"party_1_team_skills\"])\n",
    "        party_1_team_salaries = ensure_2d(obs_dict[\"party_1_team_salaries\"])\n",
    "        party_2_team_skills = ensure_2d(obs_dict[\"party_2_team_skills\"])\n",
    "        party_2_team_salaries = ensure_2d(obs_dict[\"party_2_team_salaries\"])\n",
    "        party_1_candidate = ensure_2d(obs_dict[\"party_1_candidate\"])\n",
    "        party_2_candidate = ensure_2d(obs_dict[\"party_2_candidate\"])\n",
    "        \n",
    "        # Concatenate all features\n",
    "        return torch.cat([\n",
    "            collab_mode,\n",
    "            party_1_status,\n",
    "            party_2_status,\n",
    "            party_1_resources,\n",
    "            party_2_resources,\n",
    "            party_1_team_skills,\n",
    "            party_1_team_salaries,\n",
    "            party_2_team_skills,\n",
    "            party_2_team_salaries,\n",
    "            party_1_candidate,\n",
    "            party_2_candidate\n",
    "        ], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "Step: 0 | Collaboration Mode: 0\n",
      "Party 1 Status: 0 | Resources: 100.00\n",
      "Party 2 Status: 0 | Resources: 100.00\n",
      "Party 1 Team Skill: 1.57 | Salary: 1.28\n",
      "Party 2 Team Skill: 0.53 | Salary: 0.86\n",
      "Party 1 Team:\n",
      "  Worker 0: Skill=0.91, Salary=0.56\n",
      "  Worker 1: Skill=0.66, Salary=0.72\n",
      "  Worker 2: Skill=0.00, Salary=0.00\n",
      "Party 2 Team:\n",
      "  Worker 0: Skill=0.00, Salary=0.05\n",
      "  Worker 1: Skill=0.00, Salary=0.15\n",
      "  Worker 2: Skill=0.53, Salary=0.66\n",
      "Party 1 Candidate: Skill=0.38, Salary=0.65\n",
      "Party 2 Candidate: Skill=0.48, Salary=0.38\n",
      "\n",
      "Action Space: Tuple(Discrete(7), Discrete(7))\n",
      "Observation Space: Dict('collab_mode': Discrete(3), 'party_1_candidate': Box(0.0, 1.0, (2,), float32), 'party_1_resources': Box(0.0, inf, (1,), float32), 'party_1_status': Discrete(3), 'party_1_team_salaries': Box(0.0, 1.0, (3,), float32), 'party_1_team_skills': Box(0.0, 1.0, (3,), float32), 'party_2_candidate': Box(0.0, 1.0, (2,), float32), 'party_2_resources': Box(0.0, inf, (1,), float32), 'party_2_status': Discrete(3), 'party_2_team_salaries': Box(0.0, 1.0, (3,), float32), 'party_2_team_skills': Box(0.0, 1.0, (3,), float32))\n",
      "\n",
      "Step 1:\n",
      "Actions: Party 1 = 0, Party 2 = 1\n",
      "Rewards: Party 1 = 10.00, Party 2 = 0.00\n",
      "Done: True\n",
      "Step: 1 | Collaboration Mode: 0\n",
      "Party 1 Status: 1 | Resources: 110.00\n",
      "Party 2 Status: 0 | Resources: 99.00\n",
      "Party 1 Team Skill: 1.57 | Salary: 1.28\n",
      "Party 2 Team Skill: 0.53 | Salary: 0.86\n",
      "Party 1 Team:\n",
      "  Worker 0: Skill=0.91, Salary=0.56\n",
      "  Worker 1: Skill=0.66, Salary=0.72\n",
      "  Worker 2: Skill=0.00, Salary=0.00\n",
      "Party 2 Team:\n",
      "  Worker 0: Skill=0.00, Salary=0.05\n",
      "  Worker 1: Skill=0.00, Salary=0.15\n",
      "  Worker 2: Skill=0.53, Salary=0.66\n",
      "Party 1 Candidate: Skill=0.38, Salary=0.65\n",
      "Party 2 Candidate: Skill=0.48, Salary=0.38\n",
      "\n",
      "Episode ended early.\n",
      "Terminal Info: {'retreated': 'party_1', 'party_1_skill': np.float64(1.573906603082572), 'party_2_skill': np.float64(0.5316345859058824), 'party_1_salary': np.float64(1.2766018956036613), 'party_2_salary': np.float64(0.8580825948284105)}\n",
      "\n",
      "\n",
      "Testing collaboration scenario:\n",
      "After collaboration initiation:\n",
      "Collaboration Mode: 1\n",
      "Rewards: Party 1 = 0.00, Party 2 = 0.00\n",
      "Step: 1 | Collaboration Mode: 1\n",
      "Party 1 Status: 0 | Resources: 92.00\n",
      "Party 2 Status: 0 | Resources: 92.00\n",
      "Party 1 Team Skill: 2.48 | Salary: 2.73\n",
      "Party 2 Team Skill: 2.32 | Salary: 2.54\n",
      "Party 1 Team:\n",
      "  Worker 0: Skill=0.82, Salary=1.00\n",
      "  Worker 1: Skill=0.67, Salary=0.74\n",
      "  Worker 2: Skill=1.00, Salary=1.00\n",
      "Party 2 Team:\n",
      "  Worker 0: Skill=0.85, Salary=0.76\n",
      "  Worker 1: Skill=0.76, Salary=0.95\n",
      "  Worker 2: Skill=0.70, Salary=0.83\n",
      "Party 1 Candidate: Skill=0.77, Salary=0.45\n",
      "Party 2 Candidate: Skill=0.88, Salary=0.44\n",
      "\n",
      "\n",
      "Testing hiring action:\n",
      "After hiring action:\n",
      "Rewards: Party 1 = 0.39, Party 2 = 0.00\n",
      "Step: 1 | Collaboration Mode: 0\n",
      "Party 1 Status: 0 | Resources: 100.00\n",
      "Party 2 Status: 0 | Resources: 99.00\n",
      "Party 1 Team Skill: 2.00 | Salary: 2.22\n",
      "Party 2 Team Skill: 1.93 | Salary: 1.95\n",
      "Party 1 Team:\n",
      "  Worker 0: Skill=1.00, Salary=1.00\n",
      "  Worker 1: Skill=1.00, Salary=1.00\n",
      "  Worker 2: Skill=0.00, Salary=0.22\n",
      "Party 2 Team:\n",
      "  Worker 0: Skill=0.94, Salary=1.00\n",
      "  Worker 1: Skill=0.00, Salary=0.14\n",
      "  Worker 2: Skill=0.98, Salary=0.82\n",
      "Party 1 Candidate: Skill=0.00, Salary=0.12\n",
      "Party 2 Candidate: Skill=1.00, Salary=0.66\n"
     ]
    }
   ],
   "source": [
    "def test_integrated_environment():\n",
    "    \"\"\"Test the integrated environment functionality.\"\"\"\n",
    "    env = CompetitiveHiringAGIEnv(team_size=3)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    env.render()\n",
    "    print(\"\\nAction Space:\", env.action_space)\n",
    "    print(\"Observation Space:\", env.observation_space)\n",
    "    \n",
    "    # Test a few steps with random actions\n",
    "    for step in range(5):\n",
    "        action = (\n",
    "            np.random.randint(0, 7),  # Random action for party 1 (0-6 for team_size=3)\n",
    "            np.random.randint(0, 7)   # Random action for party 2\n",
    "        )\n",
    "        next_obs, rewards, done, _, info = env.step(action)\n",
    "        \n",
    "        print(f\"\\nStep {step+1}:\")\n",
    "        print(f\"Actions: Party 1 = {action[0]}, Party 2 = {action[1]}\")\n",
    "        print(f\"Rewards: Party 1 = {rewards[0]:.2f}, Party 2 = {rewards[1]:.2f}\")\n",
    "        print(f\"Done: {done}\")\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\nEpisode ended early.\")\n",
    "            print(\"Terminal Info:\", info)\n",
    "            break\n",
    "\n",
    "    # Test collaboration scenario\n",
    "    print(\"\\n\\nTesting collaboration scenario:\")\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Both parties initiate collaboration\n",
    "    action = (2, 2)\n",
    "    next_obs, rewards, done, _, info = env.step(action)\n",
    "    print(\"After collaboration initiation:\")\n",
    "    print(f\"Collaboration Mode: {next_obs['collab_mode']}\")\n",
    "    print(f\"Rewards: Party 1 = {rewards[0]:.2f}, Party 2 = {rewards[1]:.2f}\")\n",
    "    env.render()\n",
    "    \n",
    "    # Test hiring action\n",
    "    print(\"\\n\\nTesting hiring action:\")\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Party 1 hires a new worker (replacing worker 0)\n",
    "    action = (4, 1)  # Party 1: Hire (fire index 0), Party 2: Explore\n",
    "    next_obs, rewards, done, _, info = env.step(action)\n",
    "    print(\"After hiring action:\")\n",
    "    print(f\"Rewards: Party 1 = {rewards[0]:.2f}, Party 2 = {rewards[1]:.2f}\")\n",
    "    env.render()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_integrated_environment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
