{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitiveAGIEnv(gymnasium.Env):\n",
    "    \"\"\"Competitive AGI Race Environment with Two Parties and State Validation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CompetitiveAGIEnv, self).__init__()\n",
    "\n",
    "        # Defining collaboration modes\n",
    "        self.COLLAB_MODES = {\n",
    "            'INDEPENDENT': 0,   # Both parties working independently (never collaborated)\n",
    "            'COLLABORATIVE': 1, # Both parties actively collaborating\n",
    "            'POST_COLLAB': 2,   # Both previously collaborated but now independent\n",
    "        }\n",
    "        \n",
    "        # Defining party statuses\n",
    "        self.PARTY_STATUS = {\n",
    "            'EXPLORING': 0,     # Actively exploring\n",
    "            'RETREATED': 1,     # Retreated\n",
    "            'FOUND_AGI': 2,     # Found AGI\n",
    "        }\n",
    "        \n",
    "        # Collaboration mode (applies to both parties)\n",
    "        self.collab_mode = self.COLLAB_MODES['INDEPENDENT']\n",
    "        \n",
    "        # Individual party statuses\n",
    "        self.party_1_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_2_status = self.PARTY_STATUS['EXPLORING']\n",
    "        \n",
    "        # Track which party has broken collaboration\n",
    "        self.party_1_broke_collab = False\n",
    "        self.party_2_broke_collab = False\n",
    "        \n",
    "        # Track resources\n",
    "        self.party_1_resources = 100\n",
    "        self.party_2_resources = 100\n",
    "        \n",
    "        # 4 actions for each party:\n",
    "        # 0 = Explore independently\n",
    "        # 1 = Retreat (take a break)\n",
    "        # 2 = Initiate/continue collaboration\n",
    "        # 3 = Break collaboration (if currently collaborating)\n",
    "        # Each party chooses one action, so the combined action space is (4, 4)\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(4), spaces.Discrete(4)))\n",
    "        \n",
    "        # The observation space includes:\n",
    "        # - Collaboration mode (0, 1, 2)\n",
    "        # - Party 1 status (0, 1, 2)\n",
    "        # - Party 2 status (0, 1, 2)\n",
    "        # - Party 1 resources (float)\n",
    "        # - Party 2 resources (float)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(3),  # collaboration mode\n",
    "            spaces.Discrete(3),  # party_1_status\n",
    "            spaces.Discrete(3),  # party_2_status\n",
    "            spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),  # party_1_resources\n",
    "            spaces.Box(low=0, high=float('inf'), shape=(1,), dtype=np.float32),  # party_2_resources\n",
    "        ))\n",
    "        \n",
    "        # Base probabilities for AGI discovery\n",
    "        self.independent_agi_prob = 0.01\n",
    "        self.collaborative_agi_prob = 0.03\n",
    "        self.post_collab_agi_prob = 0.015\n",
    "        \n",
    "        # Resource dynamics\n",
    "        self.explore_independent_cost = 1\n",
    "        self.explore_collaborative_cost = 3\n",
    "        self.retreat_reward = 10\n",
    "        self.collaboration_initiation_cost = 5\n",
    "        self.agi_reward = 100\n",
    "        \n",
    "        # Competition penalty: if the opponent finds AGI first\n",
    "        self.competition_penalty = -50\n",
    "        \n",
    "        # Max environment steps\n",
    "        self.max_steps = 1000\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Defining the transition probabilities and rewards for each party\n",
    "        # Structure: {(collab_mode, party_status): {action: [{next_status, prob, reward, effects}, ...]}}\n",
    "        self.party_1_transitions = {\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['INDEPENDENT']): {\n",
    "                0: [  # Explore independently\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": self.independent_agi_prob, \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_2_reward\", self.competition_penalty, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1 - self.independent_agi_prob, \n",
    "                        \"reward\": -1, \"resource_change\": -self.explore_independent_cost, \"effects\": []}                        #1. CHANGED REWARD FROM 0 TO -1\n",
    "                ],\n",
    "                1: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Try to initiate collaboration. COLLABORATIVE CASE ALREADY HANDLED EARLIER\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                        \"reward\": -5, \"resource_change\": 0, \"effects\": []}  # Actual collab handled separately                 #3. CHANGED REWARD FROM 0 TO -5\n",
    "                ],\n",
    "                # 3: [  # Invalid in this state                                                                             #2. REMOVED INVALID ACTIONS\n",
    "                #     {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                #      \"reward\": -5, \"resource_change\": 0, \"effects\": []}  # Penalty for invalid action\n",
    "                # ]\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['COLLABORATIVE']): {\n",
    "                # 0: [  # Explore collaboratively                                                                           #2. REMOVED INVALID ACTIONS\n",
    "                #     {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": self.collaborative_agi_prob, \n",
    "                #      \"reward\": self.agi_reward, \"resource_change\": -self.explore_collaborative_cost,\n",
    "                #      \"effects\": [(\"party_2_status\", self.PARTY_STATUS['FOUND_AGI'], \n",
    "                #                  lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING']),\n",
    "                #                 (\"party_2_reward\", self.agi_reward, \n",
    "                #                  lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                #     {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1 - self.collaborative_agi_prob, \n",
    "                #      \"reward\": 0, \"resource_change\": -self.explore_collaborative_cost, \"effects\": []}\n",
    "                # ],\n",
    "                1: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                2: [  # Explore collaboratively\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": self.collaborative_agi_prob, \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_collaborative_cost,\n",
    "                        \"effects\": [(\"party_2_status\", self.PARTY_STATUS['FOUND_AGI'], \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING']),\n",
    "                                (\"party_2_reward\", self.agi_reward, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1 - self.collaborative_agi_prob, \n",
    "                        \"reward\": -5, \"resource_change\": -self.explore_collaborative_cost, \"effects\": []}                      #3. CHANGED REWARD FROM 0 TO -5\n",
    "                ],\n",
    "                3: [  # Break collaboration, COLLABORATIVE CASE ALREADY HANDLED EARLIER\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                        \"reward\": 0, \"resource_change\": 0, \"effects\": [(\"party_1_broke_collab\", True, lambda: True)]}\n",
    "                ]\n",
    "            },\n",
    "            (self.PARTY_STATUS['EXPLORING'], self.COLLAB_MODES['POST_COLLAB']): {\n",
    "                0: [  # Explore after collaboration\n",
    "                    {\"next_status\": self.PARTY_STATUS['FOUND_AGI'], \"prob\": self.post_collab_agi_prob, \n",
    "                        \"reward\": self.agi_reward, \"resource_change\": -self.explore_independent_cost,\n",
    "                        \"effects\": [(\"party_2_reward\", self.competition_penalty, \n",
    "                                    lambda: self.party_2_status == self.PARTY_STATUS['EXPLORING'])]},\n",
    "                    {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1 - self.post_collab_agi_prob, \n",
    "                        \"reward\": -1, \"resource_change\": -self.explore_independent_cost, \"effects\": []}                        #1. CHANGED REWARD FROM 0 TO -1\n",
    "                ],\n",
    "                1: [  # Retreat\n",
    "                    {\"next_status\": self.PARTY_STATUS['RETREATED'], \"prob\": 1.0, \n",
    "                        \"reward\": self.retreat_reward, \"resource_change\": self.retreat_reward, \"effects\": []}\n",
    "                ],\n",
    "                # 2: [  # Invalid in this state \n",
    "                #     {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                #      \"reward\": 0, \"resource_change\": 0, \"effects\": []}  # Actual collab handled separately\n",
    "                # ],\n",
    "                # 3: [  # Invalid in this state                                                                                 #2. REMOVED INVALID ACTIONS\n",
    "                #     {\"next_status\": self.PARTY_STATUS['EXPLORING'], \"prob\": 1.0, \n",
    "                #      \"reward\": -5, \"resource_change\": 0, \"effects\": []}  # Penalty for invalid action\n",
    "                # ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Define similar transitions for party 2 (could be identical or asymmetric)\n",
    "        self.party_2_transitions = self.party_1_transitions.copy()  # Deep copy if needed\n",
    "\n",
    "        # Define joint collaboration transitions\n",
    "        self.collaboration_transitions = {\n",
    "            self.COLLAB_MODES['INDEPENDENT']: {\n",
    "                (2, 2): {\"next_mode\": self.COLLAB_MODES['COLLABORATIVE'], \"resource_change\": -self.collaboration_initiation_cost}\n",
    "            },\n",
    "            self.COLLAB_MODES['COLLABORATIVE']: {\n",
    "                (2, 2): {\"next_mode\": self.COLLAB_MODES['COLLABORATIVE'], \"resource_change\": -5},\n",
    "                (3, 2): {\"next_mode\": self.COLLAB_MODES['POST_COLLAB'], \"resource_change\": -1},\n",
    "                (2, 3): {\"next_mode\": self.COLLAB_MODES['POST_COLLAB'], \"resource_change\": -1},\n",
    "                (3, 3): {\"next_mode\": self.COLLAB_MODES['POST_COLLAB'], \"resource_change\": -1},\n",
    "                # # Any other action combination breaks collaboration\n",
    "                # \"default\": {\"next_mode\": self.COLLAB_MODES['POST_COLLAB'], \"resource_change\": 0}\n",
    "            },\n",
    "            # self.COLLAB_MODES['POST_COLLAB']: {\n",
    "            #     (2, 2): {\"next_mode\": self.COLLAB_MODES['COLLABORATIVE'], \"resource_change\": -self.collaboration_initiation_cost}\n",
    "            # }\n",
    "        }\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Return the current observation (state).\"\"\"\n",
    "        return (\n",
    "            self.collab_mode,\n",
    "            self.party_1_status,\n",
    "            self.party_2_status,\n",
    "            np.array([self.party_1_resources], dtype=np.float32),\n",
    "            np.array([self.party_2_resources], dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def _is_terminal(self):\n",
    "        \"\"\"Check if the episode has reached a terminal state.\"\"\"\n",
    "        return (self.party_1_status != self.PARTY_STATUS['EXPLORING'] or \n",
    "                self.party_2_status != self.PARTY_STATUS['EXPLORING'] or \n",
    "                self.current_step >= self.max_steps or\n",
    "                self.party_1_resources <= 0 or \n",
    "                self.party_2_resources <= 0)\n",
    "\n",
    "    def _get_terminal_info(self):\n",
    "        \"\"\"Gather information about how the episode ended.\"\"\"\n",
    "        info = {}\n",
    "        \n",
    "        if self.party_1_resources <= 0:\n",
    "            info[\"bankrupt\"] = \"party_1\"\n",
    "        if self.party_2_resources <= 0:\n",
    "            info[\"bankrupt\"] = \"party_2\" if \"bankrupt\" not in info else \"both\"\n",
    "        \n",
    "        if self.current_step >= self.max_steps:\n",
    "            info[\"timeout\"] = True\n",
    "            \n",
    "        if self.party_1_status == self.PARTY_STATUS['FOUND_AGI'] and self.party_2_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"both\"\n",
    "        elif self.party_1_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"party_1\"\n",
    "        elif self.party_2_status == self.PARTY_STATUS['FOUND_AGI']:\n",
    "            info[\"winner\"] = \"party_2\"\n",
    "        \n",
    "        if \"winner\" not in info and \"bankrupt\" not in info and not info.get(\"timeout\", False):\n",
    "            # Someone retreated\n",
    "            if self.party_1_status == self.PARTY_STATUS['RETREATED'] and self.party_2_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"both_retreated\"] = True\n",
    "            elif self.party_1_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"retreated\"] = \"party_1\"\n",
    "            elif self.party_2_status == self.PARTY_STATUS['RETREATED']:\n",
    "                info[\"retreated\"] = \"party_2\"\n",
    "                \n",
    "        return info\n",
    "\n",
    "    # def _process_collaboration_actions(self, action_1, action_2):\n",
    "    #     \"\"\"Process collaboration dynamics based on both parties' actions.\n",
    "    #         HERE, True and False JUST INDICATE WHETHER WE MADE SOME CHANGE BASED ON THEIR STATE, AND COLLABORATION-RELATED ACTION OR NOT. \n",
    "    #         WE COULD HAVE AVOIDED TRUE, AND FALSE ALTOGETHER\"\"\"\n",
    "        \n",
    "    #     # Starting collaboration requires both parties to choose action 2\n",
    "    #     if (self.collab_mode != self.COLLAB_MODES['COLLABORATIVE'] and \n",
    "    #         action_1 == 2 and action_2 == 2):\n",
    "            \n",
    "    #         # Both parties agree to collaborate\n",
    "    #         self.collab_mode = self.COLLAB_MODES['COLLABORATIVE']\n",
    "    #         self.party_1_resources -= self.collaboration_initiation_cost\n",
    "    #         self.party_2_resources -= self.collaboration_initiation_cost\n",
    "    #         return True\n",
    "            \n",
    "    #     # Breaking collaboration happens if either party chooses action 3\n",
    "    #     elif (self.collab_mode == self.COLLAB_MODES['COLLABORATIVE'] and \n",
    "    #           (action_1 == 3 or action_2 == 3)):\n",
    "            \n",
    "    #         # Collaboration is broken\n",
    "    #         self.collab_mode = self.COLLAB_MODES['POST_COLLAB']\n",
    "            \n",
    "    #         # Record who broke collaboration\n",
    "    #         if action_1 == 3:\n",
    "    #             self.party_1_broke_collab = True\n",
    "    #         if action_2 == 3:\n",
    "    #             self.party_2_broke_collab = True\n",
    "    #         return True\n",
    "            \n",
    "    #     # Continuing collaboration requires both to choose action 2\n",
    "    #     elif (self.collab_mode == self.COLLAB_MODES['COLLABORATIVE'] and \n",
    "    #           not (action_1 == 2 and action_2 == 2)):\n",
    "            \n",
    "    #         # Collaboration ends (not actively broken, but not continued)\n",
    "    #         self.collab_mode = self.COLLAB_MODES['POST_COLLAB']\n",
    "    #         return True\n",
    "            \n",
    "    #     return False\n",
    "    \n",
    "    def _process_collaboration_actions(self, action_1, action_2):\n",
    "        collab_action_key = (action_1, action_2)\n",
    "        \n",
    "        # Apply collaboration transition if defined for this action pair\n",
    "        if self.collab_mode in self.collaboration_transitions:\n",
    "            collab_state_transitions = self.collaboration_transitions[self.collab_mode]\n",
    "            \n",
    "            if collab_action_key in collab_state_transitions:\n",
    "                transition = collab_state_transitions[collab_action_key]\n",
    "                \n",
    "                # Apply collaboration transition\n",
    "                old_collab_mode = self.collab_mode\n",
    "                self.collab_mode = transition[\"next_mode\"]\n",
    "                \n",
    "                # Apply resource changes for collaboration transitions\n",
    "                if transition[\"resource_change\"] != 0:\n",
    "                    self.party_1_resources += transition[\"resource_change\"]\n",
    "                    self.party_2_resources += transition[\"resource_change\"]\n",
    "                \n",
    "                # Record collaboration break\n",
    "                if old_collab_mode == self.COLLAB_MODES['COLLABORATIVE'] and self.collab_mode == self.COLLAB_MODES['POST_COLLAB']:\n",
    "                    if action_1 == 3:\n",
    "                        self.party_1_broke_collab = True\n",
    "                    if action_2 == 3:\n",
    "                        self.party_2_broke_collab = True\n",
    "\n",
    "    def _get_agi_probability(self, party_id):\n",
    "        \"\"\"Get the probability of finding AGI based on collaboration mode.\"\"\"\n",
    "        if self.collab_mode == self.COLLAB_MODES['INDEPENDENT']:\n",
    "            return self.independent_agi_prob\n",
    "        elif self.collab_mode == self.COLLAB_MODES['COLLABORATIVE']:\n",
    "            return self.collaborative_agi_prob\n",
    "        else:  # POST_COLLAB\n",
    "            return self.post_collab_agi_prob\n",
    "        \n",
    "    def _process_party1_actions(self, action_1, reward_1, reward_2):\n",
    "        # Process party 1's action if not in terminal state\n",
    "        if self.party_1_status == self.PARTY_STATUS['EXPLORING']:\n",
    "            state_key = (self.party_1_status, self.collab_mode)\n",
    "            \n",
    "            if state_key in self.party_1_transitions and action_1 in self.party_1_transitions[state_key]:\n",
    "                transitions = self.party_1_transitions[state_key][action_1]\n",
    "                \n",
    "                # Select transition based on probabilities\n",
    "                probs = [t[\"prob\"] for t in transitions]\n",
    "                transition_idx = np.random.choice(len(transitions), p=probs)\n",
    "                transition = transitions[transition_idx]\n",
    "                \n",
    "                # Apply transition\n",
    "                self.party_1_status = transition[\"next_status\"]\n",
    "                reward_1 += transition[\"reward\"]\n",
    "                self.party_1_resources += transition[\"resource_change\"]\n",
    "                \n",
    "                # Apply side effects\n",
    "                for effect in transition[\"effects\"]:\n",
    "                    target, value, condition = effect\n",
    "                    if condition():\n",
    "                        if target == \"party_2_reward\":\n",
    "                            reward_2 += value\n",
    "                        elif target == \"party_2_status\":\n",
    "                            self.party_2_status = value\n",
    "                        elif target == \"party_1_broke_collab\":\n",
    "                            self.party_1_broke_collab = value\n",
    "    \n",
    "    def _process_party2_actions(self, action_2, reward_1, reward_2):\n",
    "        # Process party 2's action if not in terminal state and if party 1's action didn't\n",
    "        # already cause party 2 to find AGI through collaboration\n",
    "        if self.party_2_status == self.PARTY_STATUS['EXPLORING']:\n",
    "            state_key = (self.party_2_status, self.collab_mode)\n",
    "            \n",
    "            if state_key in self.party_2_transitions and action_2 in self.party_2_transitions[state_key]:\n",
    "                transitions = self.party_2_transitions[state_key][action_2]\n",
    "                \n",
    "                # Select transition based on probabilities\n",
    "                probs = [t[\"prob\"] for t in transitions]\n",
    "                transition_idx = np.random.choice(len(transitions), p=probs)\n",
    "                transition = transitions[transition_idx]\n",
    "                \n",
    "                # Apply transition\n",
    "                self.party_2_status = transition[\"next_status\"]\n",
    "                reward_2 += transition[\"reward\"]\n",
    "                self.party_2_resources += transition[\"resource_change\"]\n",
    "                \n",
    "                # Apply side effects\n",
    "                for effect in transition[\"effects\"]:\n",
    "                    target, value, condition = effect\n",
    "                    if condition():\n",
    "                        if target == \"party_1_reward\":\n",
    "                            reward_1 += value\n",
    "                        elif target == \"party_1_status\":\n",
    "                            self.party_1_status = value\n",
    "                        elif target == \"party_2_broke_collab\":\n",
    "                            self.party_2_broke_collab = value\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment with actions from both parties.\"\"\"\n",
    "        \"\"\"Take a step using transition matrices instead of if-else logic.\"\"\"\n",
    "        action_1, action_2 = action\n",
    "        \n",
    "        # Validate actions\n",
    "        assert 0 <= action_1 < 4, f\"Invalid action for party 1: {action_1}\"\n",
    "        assert 0 <= action_2 < 4, f\"Invalid action for party 2: {action_2}\"\n",
    "        \n",
    "        # If already in terminal state, return without changes\n",
    "        if self._is_terminal():\n",
    "            return self._get_observation(), (0, 0), True, False, self._get_terminal_info()\n",
    "        \n",
    "        # Process collaboration transitions first\n",
    "        self._process_collaboration_actions(action_1, action_2)\n",
    "        \n",
    "        reward_1, reward_2 = 0, 0\n",
    "        info = {}\n",
    "\n",
    "        self._process_party1_actions(action_1, reward_1, reward_2)\n",
    "        self._process_party2_actions(action_2, reward_1, reward_2)\n",
    "\n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Determine if episode has ended\n",
    "        done = self._is_terminal()\n",
    "        \n",
    "        # Gather additional info for terminal states\n",
    "        if done:\n",
    "            info.update(self._get_terminal_info())\n",
    "        \n",
    "        return self._get_observation(), (reward_1, reward_2), done, False, info\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.collab_mode = self.COLLAB_MODES['INDEPENDENT']\n",
    "        self.party_1_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_2_status = self.PARTY_STATUS['EXPLORING']\n",
    "        self.party_1_broke_collab = False\n",
    "        self.party_2_broke_collab = False\n",
    "        self.party_1_resources = 100\n",
    "        self.party_2_resources = 100\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Environment Test ===\n",
      "Initial Observation: (0, 0, 0, array([100.], dtype=float32), array([100.], dtype=float32))\n",
      "Action Space: Tuple(Discrete(4), Discrete(4))\n",
      "Observation Space: Tuple(Discrete(3), Discrete(3), Discrete(3), Box(0.0, inf, (1,), float32), Box(0.0, inf, (1,), float32))\n",
      "\n",
      "Took action: (2, 2)\n",
      "Next Observation: (1, 0, 0, array([92.], dtype=float32), array([92.], dtype=float32))\n",
      "Rewards: (0, 0)\n",
      "Done: False\n",
      "Info: {}\n",
      "\n",
      "Episode ended after 6 steps\n",
      "Final Observation: (0, 0, 1, array([97.], dtype=float32), array([110.], dtype=float32))\n",
      "Final Info: {'retreated': 'party_2'}\n",
      "\n",
      "=== Collaboration Dynamics Test ===\n",
      "Testing collaboration initiation...\n",
      "Collaboration Mode: 1\n",
      "Resources P1: 92.0, P2: 92.0\n",
      "✓ Collaboration successfully initiated\n",
      "\n",
      "Both parties explored while collaborating:\n",
      "Resources P1: 92.0, P2: 92.0\n",
      "\n",
      "Testing collaboration breaking...\n",
      "Collaboration Mode: 1\n",
      "✗ Collaboration failed to break\n",
      "Party 1 broke collab: True\n",
      "Party 2 broke collab: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# First, make sure your CompetitiveAGIEnv class is defined correctly\n",
    "# Then, run these tests:\n",
    "\n",
    "def test_basic_environment():\n",
    "    \"\"\"Test basic environment functionality.\"\"\"\n",
    "    env = CompetitiveAGIEnv()\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(\"Initial Observation:\", obs)\n",
    "    print(\"Action Space:\", env.action_space)\n",
    "    print(\"Observation Space:\", env.observation_space)\n",
    "    \n",
    "    # Test a single step with random actions\n",
    "    action = (random.randint(0, 3), random.randint(0, 3))\n",
    "    next_obs, rewards, done, _, info = env.step(action)\n",
    "    \n",
    "    print(f\"\\nTook action: {action}\")\n",
    "    print(\"Next Observation:\", next_obs)\n",
    "    print(\"Rewards:\", rewards)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    \n",
    "    # Reset and run a full episode with random actions\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_steps = 0\n",
    "    \n",
    "    while not done and total_steps < 100:\n",
    "        action = (random.randint(0, 3), random.randint(0, 3))\n",
    "        obs, rewards, done, _, info = env.step(action)\n",
    "        total_steps += 1\n",
    "    \n",
    "    print(f\"\\nEpisode ended after {total_steps} steps\")\n",
    "    print(\"Final Observation:\", obs)\n",
    "    print(\"Final Info:\", info)\n",
    "\n",
    "\n",
    "def test_collaboration_dynamics():\n",
    "    \"\"\"Test collaboration-specific dynamics.\"\"\"\n",
    "    env = CompetitiveAGIEnv()\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Test initiating collaboration\n",
    "    print(\"Testing collaboration initiation...\")\n",
    "    action = (2, 2)  # Both parties choose to collaborate\n",
    "    next_obs, rewards, done, _, info = env.step(action)\n",
    "    \n",
    "    print(f\"Collaboration Mode: {next_obs[0]}\")\n",
    "    print(f\"Resources P1: {next_obs[3][0]}, P2: {next_obs[4][0]}\")\n",
    "    \n",
    "    if next_obs[0] == env.COLLAB_MODES['COLLABORATIVE']:\n",
    "        print(\"✓ Collaboration successfully initiated\")\n",
    "    else:\n",
    "        print(\"✗ Collaboration failed to initiate\")\n",
    "    \n",
    "    # Test collaborative exploration\n",
    "    if not done:\n",
    "        action = (0, 0)  # Both parties explore while in collaboration\n",
    "        next_obs, rewards, done, _, info = env.step(action)\n",
    "        print(\"\\nBoth parties explored while collaborating:\")\n",
    "        print(f\"Resources P1: {next_obs[3][0]}, P2: {next_obs[4][0]}\")\n",
    "    \n",
    "    # Test breaking collaboration\n",
    "    env.reset()\n",
    "    # First initiate collaboration\n",
    "    env.step((2, 2))\n",
    "    \n",
    "    # Then break it\n",
    "    print(\"\\nTesting collaboration breaking...\")\n",
    "    action = (3, 0)  # Party 1 breaks collaboration\n",
    "    next_obs, rewards, done, _, info = env.step(action)\n",
    "    \n",
    "    print(f\"Collaboration Mode: {next_obs[0]}\")\n",
    "    if next_obs[0] == env.COLLAB_MODES['POST_COLLAB']:\n",
    "        print(\"✓ Collaboration successfully broken\")\n",
    "    else:\n",
    "        print(\"✗ Collaboration failed to break\")\n",
    "    \n",
    "    # Test if broke_collab flag is set\n",
    "    print(f\"Party 1 broke collab: {env.party_1_broke_collab}\")\n",
    "    print(f\"Party 2 broke collab: {env.party_2_broke_collab}\")\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Basic Environment Test ===\")\n",
    "    test_basic_environment()\n",
    "    \n",
    "    print(\"\\n=== Collaboration Dynamics Test ===\")\n",
    "    test_collaboration_dynamics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
